{"cells":[{"cell_type":"markdown","metadata":{"id":"hNWNgXa_Ti0j"},"source":["# Dataset"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1701745780870,"user":{"displayName":"Jemuel Premkumar","userId":"15271811269029043422"},"user_tz":300},"id":"H_9ijeD6pC0S"},"outputs":[],"source":["%load_ext autoreload\n","%autoreload 2"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16798,"status":"ok","timestamp":1701745797660,"user":{"displayName":"Jemuel Premkumar","userId":"15271811269029043422"},"user_tz":300},"id":"7dqFXOnopFZQ","outputId":"868c595c-5348-479f-e65e-ddb8b31f7bc0"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n","['data', 'requirements.txt', 'utils', 'params.json', 'reconstruction.py', 'hello_testing.py', '__pycache__', 'PCN.ipynb', 'Reconstruct.ipynb', 'Grasp Generation.ipynb', 'dataset.py', '.ipynb_checkpoints', 'config_files', 'model', 'results', 'scripts', 'obj_paths.pkl', 'Dataset.ipynb', 'eval']\n"]}],"source":["import os\n","from google.colab import drive\n","\n","drive.mount('/content/drive')\n","\n","# TODO: Fill this according to your drive loc\n","# DAVID: GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = 'DeepSDF/'\n","# STANLEY: GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = 'ROB 498 3D Robot Perception/3D-RP-Project/DeepSDF/'\n","# DEEPAK: GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = 'DeepSDF/'\n","GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = 'ROB 498 3D Robot Perception/3D-RP-Project/DeepSDF/'\n","GOOGLE_DRIVE_PATH = os.path.join('drive', 'My Drive', GOOGLE_DRIVE_PATH_AFTER_MYDRIVE)\n","print(os.listdir(GOOGLE_DRIVE_PATH))"]},{"cell_type":"markdown","metadata":{"id":"Bo9Z1jjfpOgz"},"source":["If you did everything right, you should see something like this:\n","```\n","['DeepSDF.ipynb', 'data', 'models', 'requirements.txt', 'utils', 'params.json', 'Reconstruct.ipynb', 'Dataset.ipynb', 'hello_testing.py', 'model.py', '__pycache__']\n","```"]},{"cell_type":"markdown","metadata":{"id":"I3M5AyeJt56S"},"source":["<font color='red'>NOTE:</font> You don't have to clone the shared drive.\n","1. Create a shortcut of the shared folder (DeepSDF) and place in your drive.\n","2. Then change the directory on the notebook according to your gdrive."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":624,"status":"ok","timestamp":1701740816122,"user":{"displayName":"Jemuel Stanley","userId":"07835643054669309596"},"user_tz":300},"id":"-LxLo8LVpMyQ","outputId":"d85330f9-22e7-4595-d6cd-541ed4755f8b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Working!\n","dataset.py last edited on Tue Nov 21 21:22:54 2023\n"]}],"source":["import sys\n","sys.path.append(GOOGLE_DRIVE_PATH)\n","\n","import time, os\n","os.environ[\"TZ\"] = \"US/Eastern\"\n","time.tzset()\n","\n","from hello_testing import hello\n","hello()\n","\n","dataset_path = os.path.join(GOOGLE_DRIVE_PATH, 'dataset.py')\n","dataset_edit_time = time.ctime(os.path.getmtime(dataset_path))\n","print('dataset.py last edited on %s' % dataset_edit_time)"]},{"cell_type":"markdown","metadata":{"id":"lyhoHkMLpTMa"},"source":["Navigate to :\n","\n","```drive > .... > 3D-RP-Project > dataset.py```\n","\n","Double-click on the file to open it on the side."]},{"cell_type":"markdown","metadata":{"id":"Z433Uuahk23t"},"source":["## Remaining in reconstruction.py"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6155,"status":"ok","timestamp":1701740822268,"user":{"displayName":"Jemuel Stanley","userId":"07835643054669309596"},"user_tz":300},"id":"yuQgwcuX98Gk","outputId":"aae51e71-35a7-464b-b7b6-10ae4fa7ad8a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting trimesh\n","  Downloading trimesh-4.0.5-py3-none-any.whl (688 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/688.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.4/688.5 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m399.4/688.5 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m686.1/688.5 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m688.5/688.5 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from trimesh) (1.23.5)\n","Installing collected packages: trimesh\n","Successfully installed trimesh-4.0.5\n"]}],"source":["!pip install trimesh"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H7GlTVtr9yOS"},"outputs":[],"source":["import torch\n","import os\n","import model.model_sdf as sdf_model\n","from utils import utils_deepsdf\n","import trimesh\n","from results import runs_sdf\n","import results\n","import numpy as np\n","import config_files\n","import yaml"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DgZ7ixj594dn"},"outputs":[],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Puq_OAsB-Gcc"},"outputs":[],"source":["def read_params(cfg):\n","    \"\"\"Read the settings from the settings.yaml file. These are the settings used during training.\"\"\"\n","    training_settings_path = os.path.join(os.path.dirname(runs_sdf.__file__),  cfg['folder_sdf'], 'settings.yaml')\n","    with open(training_settings_path, 'rb') as f:\n","        training_settings = yaml.load(f, Loader=yaml.FullLoader)\n","\n","    return training_settings\n","\n","\n","def reconstruct_object(cfg, latent_code, obj_idx, model, coords_batches, grad_size_axis):\n","    \"\"\"\n","    Reconstruct the object from the latent code and save the mesh.\n","    Meshes are stored as .obj files under the same folder cerated during training, for example:\n","    - runs_sdf/<datetime>/meshes_training/mesh_0.obj\n","    \"\"\"\n","    sdf = utils_deepsdf.predict_sdf(latent_code, coords_batches, model)\n","    try:\n","        vertices, faces = utils_deepsdf.extract_mesh(grad_size_axis, sdf)\n","    except:\n","        print('Mesh extraction failed')\n","        return\n","\n","    # save mesh as obj\n","    mesh_dir = os.path.join(os.path.dirname(runs_sdf.__file__), cfg['folder_sdf'], 'meshes_training')\n","    if not os.path.exists(mesh_dir):\n","        os.mkdir(mesh_dir)\n","    obj_path = os.path.join(mesh_dir, f\"mesh_{obj_idx}.obj\")\n","    trimesh.exchange.export.export_mesh(trimesh.Trimesh(vertices, faces), obj_path, file_type='obj')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":177310,"status":"ok","timestamp":1701741118302,"user":{"displayName":"Jemuel Stanley","userId":"07835643054669309596"},"user_tz":300},"id":"FITt3-WD-JzM","outputId":"12a13360-7546-4137-863d-add4e543a8db"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/utils/weight_norm.py:30: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n","  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n","/usr/local/lib/python3.10/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)\n","  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"]}],"source":["def main(cfg):\n","    training_settings = read_params(cfg)\n","\n","    # Load the model\n","    weights = os.path.join(os.path.dirname(runs_sdf.__file__), cfg['folder_sdf'], 'weights.pt')\n","\n","    model = sdf_model.SDFModel(\n","        num_layers=training_settings['num_layers'],\n","        skip_connections=training_settings['latent_size'],\n","        latent_size=training_settings['latent_size'],\n","        inner_dim=training_settings['inner_dim']).to(device)\n","    model.load_state_dict(torch.load(weights, map_location=device))\n","\n","    # Extract mesh obtained with the latent code optimised at inference\n","    coords, grad_size_axis = utils_deepsdf.get_volume_coords(cfg['resolution'])\n","    coords = coords.to(device)\n","\n","    # Split coords into batches because of memory limitations\n","    coords_batches = torch.split(coords, 100000)\n","\n","    # Load paths\n","    str2int_path = os.path.join(os.path.dirname(results.__file__), 'idx_str2int_dict.npy')\n","    results_dict_path = os.path.join(os.path.dirname(runs_sdf.__file__), cfg['folder_sdf'], 'results.npy')\n","\n","    # Load dictionaries\n","    str2int_dict = np.load(str2int_path, allow_pickle=True).item()\n","    #print(str2int_dict)\n","    results_dict = np.load(results_dict_path, allow_pickle=True).item()\n","\n","    for obj_id_path in cfg['obj_ids']:\n","        # Get object index in the results dictionary\n","        obj_idx = str2int_dict[obj_id_path]  # index in collected latent vector\n","        # Get the latent code optimised during training\n","        latent_code = results_dict['best_latent_codes'][obj_idx]\n","        latent_code = torch.tensor(latent_code).to(device)\n","\n","        reconstruct_object(cfg, latent_code, obj_idx, model, coords_batches, grad_size_axis)\n","\n","\n","if __name__ == '__main__':\n","\n","    cfg_path = os.path.join(os.path.dirname(config_files.__file__), 'reconstruct_from_latent.yaml')\n","    with open(cfg_path, 'rb') as f:\n","        cfg = yaml.load(f, Loader=yaml.FullLoader)\n","\n","    main(cfg)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
